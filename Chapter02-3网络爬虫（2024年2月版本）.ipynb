{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664a9628",
   "metadata": {},
   "source": [
    "<center><h1>网络爬虫</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e6b02",
   "metadata": {},
   "source": [
    "# 2.7 Scrapy爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcef5da",
   "metadata": {},
   "source": [
    "## 2.7.1 Scrapy爬虫概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96448d",
   "metadata": {},
   "source": [
    "Scrapy是一套基于Twisted的异步处理框架，是纯Python实现的爬虫框架，用户只需要定制开发几个模块就可以轻松地实现一个爬虫，用来抓取网页内容或者各种图片。Scrapy运行于Linux/Windows/MacOS等多种环境，具有速度快、扩展性强、使用简便等特点。即便是新手，也能迅速学会使用Scrapy编写所需要的爬虫程序。Scrapy可以在本地运行，也能部署到云端实现真正的生产级数据采集系统。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc8b7a",
   "metadata": {},
   "source": [
    "### 1．Scrapy体系架构\n",
    "![avatar](image/Chapter2_2_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621dba2e",
   "metadata": {},
   "source": [
    "### 2．Scrapy工作流\n",
    "Scrapy工作流也叫作“运行流程”或叫作“数据处理流程”，整个数据处理流程由Scrapy引擎进行控制，其主要的运行步骤如下：  \n",
    "①Scrapy引擎从调度器中取出一个链接（URL）用于接下来的抓取；  \n",
    "②Scrapy引擎把URL封装成一个请求并传给下载器；  \n",
    "③下载器把资源下载下来，并封装成应答包；  \n",
    "④爬虫解析应答包；  \n",
    "⑤如果解析出的是项目，则交给项目管道进行进一步的处理；  \n",
    "⑥如果解析出的是链接（URL），则把URL交给调度器等待抓取。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3105d",
   "metadata": {},
   "source": [
    "## 2.7.2 XPath语言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03830bdb",
   "metadata": {},
   "source": [
    "XPath（XML Path Language）是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历。简单来说，网页数据是以超文本的形式来呈现的，想要获取里面的数据，就要按照一定的规则来进行数据的处理，这种规则就叫做XPath。XPath提供了超过100个内建函数，几乎所有要定位的节点都可以用XPath来定位，在做网络爬虫时可以使用XPath提取所需的信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb786b1f",
   "metadata": {},
   "source": [
    "### 1．基本术语\n",
    "XML文档通常可以被看作一棵节点树。在XML中，有元素、属性、文本、命名空间、处理指令、注释以及文档节点等七种类型的节点，其中，元素节点是最常用的节点。下面是一个HTML文档中的代码：\n",
    "```\n",
    "<html>\n",
    "    <head><title>BigData Software</title></head>\n",
    "    <p class=\"title\"><b>BigData Software</b></p>\n",
    "    <p class=\"bigdata\">There are three famous bigdata software;and their names are\n",
    "        <a href=\"http://example.com/hadoop\" class=\"hadoop\" id=\"link1\">Hadoop</a>,\n",
    "        <a href=\"http://example.com/spark\" class=\"spark\" id=\"link2\">Spark</a>and\n",
    "        <a href=\"http://example.com/flink\" class=\"flink\" id=\"link3\"><!--Flink--></a>;\n",
    "        and they are widely used in real application.</p>\n",
    "    <p class=\"bigdata\">...</p>\n",
    "</html>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa11412",
   "metadata": {},
   "source": [
    "上面的HTML文档中，<html>是文档节点，<title>BigData Software</title>是元素节点，class=\"title\"是属性节点。节点之间存在下面几种关系：  \n",
    "（1）父节点：每个元素和属性都有一个父节点。例如，html节点是head节点和p节点的父节点；head节点是title节点的父节点；第二个p节点是中间三个a节点的父节点。  \n",
    "（2）子节点：每一个元素节点的下一个直接节点是该元素节点的子节点。每个元素节点可以有零个、一个或多个子节点。例如，title节点是head节点的子节点。  \n",
    "（3）兄弟节点：拥有相同父节点的节点，就是兄弟节点。例如，第二个p节点中的三个a节点就是兄弟节点；head节点和中间三个p节点就是兄弟节点；title节点和a节点就不是兄弟节点，因为不是同一个父节点。  \n",
    "（4）祖先节点：节点的父节点以及父节点的父节点等，称作“祖先节点”。例如，html节点和head节点是title节点的祖先节点。  \n",
    "（5）后代节点：节点的子节点以及子节点的子节点等，称作“后代节点”。例如，html节点的后代节点有head、title、b、p以及a节点。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641155b2",
   "metadata": {},
   "source": [
    "### 2．基本语法\n",
    "XML/HTML文档是由标签构成的，所有的标签都有很强的层级关系。基于这种层级关系，XPath语法能够准确定位我们所需要的信息。XPath使用路径表达式来选取XML/HTML文档中的节点，这个路径表达式和普通计算机文件系统中见到的路径表达式非常相似。在XPath语法中，我们直接使用路径来选取，再加上适当的谓语或函数进行指定，就可以准确定位到指定的节点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4688d",
   "metadata": {},
   "source": [
    "#### （1）节点选取\n",
    "XPath选取节点时，是沿着路径到达目标，下表列出了常用的表达式。\n",
    "\n",
    "![avatar](image/Chapter2_2_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1335f3",
   "metadata": {},
   "source": [
    "“/”可以理解为绝对路径，需要从根节点开始；“./”则是相对路径，可以从当前节点开始；“../”则是先返回上一节点，从上一节点开始。这与普通计算机的文件系统类似。下面给出测试这些表达式的简单实例，这里需要用到lxml中的etree库，在使用之前需要执行如下命令安装lxml库：\n",
    "> pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e010ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是实例代码：\n",
    "html_text = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <head><title>BigData Software</title></head>\n",
    "    <p class=\"title\"><b>BigData Software</b></p>\n",
    "    <p class=\"bigdata\">There are three famous bigdata software;and their names are\n",
    "      <a href=\"http://example.com/hadoop\" class=\"bigdata Hadoop\" id=\"link1\">Hadoop</a>,\n",
    "      <a href=\"http://example.com/spark\" class=\"bigdata Spark\" id=\"link2\">Spark</a>and\n",
    "      <a href=\"http://example.com/flink\" class=\"bigdata Flink\" id=\"link3\"><!--Flink--></a>;\n",
    "        and they are widely used in real application.</p>\n",
    "    <p class=\"bigdata\">others</p>\n",
    "    <p>……</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "from lxml import etree\n",
    "html = etree.HTML(html_text)\n",
    "html_data = html.xpath('body')\n",
    "print(html_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5643481",
   "metadata": {},
   "source": [
    "可以看出，html.xpath('body')的输出结果不是像HTML里面那样显示的标签，  \n",
    "其实这就是我们所要的元素，只不过我们还需要再进行一步操作，也就是使用etree中的.tostring()方法将其进行转换。  \n",
    "此外，html.xpath('body')的输出结果是一个列表，因此，我们可以使用for循环来遍历列表，具体代码如下：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in html_data:\n",
    "    print(etree.tostring(element))\n",
    "# 由于输出结果比较繁杂，这里没有给出，但是观察结果可以发现，它是标签<body>中的子节点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c396a9",
   "metadata": {},
   "source": [
    "“//”表示全局搜索，比如，“//p”可以将所有的  `<p>`  标签搜索出来。  \n",
    "“/”表示在某标签下进行搜索，只能搜索子节点，不能搜索子节点的子节点。  \n",
    " 简单来说，“//”可以进行跳级搜索，“/”只能在本级上进行搜索，不能跳跃。下面是具体实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af0fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# （1）逐级搜索\n",
    "html_data = html.xpath('/html/body/p/a')\n",
    "for element in html_data:\n",
    "  print(etree.tostring(element))\n",
    "# （2）跳级搜索\n",
    "html_data = html.xpath('//a')\n",
    "for element in html_data:\n",
    "  print(etree.tostring(element))\n",
    "\n",
    "\n",
    "\n",
    "#上面两段代码的执行结果相同，具体如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f741fe2",
   "metadata": {},
   "source": [
    "可以在方括号内添加“@”，将标签属性填进去，这样就可以准确地将含有该标签属性的部分提取出来，示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "html_data = html.xpath('//p/a[@class=\"bigdata Spark\"]')\n",
    "for element in html_data:\n",
    "    print(etree.tostring(element))\n",
    "\n",
    "# 上面代码的执行结果如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd139f4",
   "metadata": {},
   "source": [
    "#### （2）谓语\n",
    "直接使用前面介绍的方法可以定位到多数我们需要的节点，但是有时候我们需要查找某个特定的节点或者包含某个指定值的节点，就要用到谓语。谓语是被嵌在方括号中的。下表列出了一些带有谓语的路径表达式及其描述的内容!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf360e5",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面演示最后一个例子，选取所有body下，class为bigdata的p标签，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data = html.xpath('//body/p[@class=\"bigdata\"]')\n",
    "for element in html_data:\n",
    " print(etree.tostring(element))\n",
    "\n",
    "# 上面代码执行结果如下：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d879558",
   "metadata": {},
   "source": [
    "#### （3）函数\n",
    "XPath中提供超过100个内建函数用于字符串值、数值、日期和时间比较序列处理等操作，极大地方便了我们定位获取所需要的信息。表3-5列出了几个常用的函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd42d07",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aca0a2",
   "metadata": {},
   "source": [
    "下面是示例代码，获取所有class属性包含bigdata的a标签中的文本内容，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d241e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = etree.HTML(html_text)\n",
    "html_data = html.xpath('//a[contains(@class, \"bigdata\")]/text()')\n",
    "print(html_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697801c",
   "metadata": {},
   "source": [
    "在演示的HTML代码中，还有一个a标签也符合代码的要求，但是因为其文本内容是注释，所以不会被抽取出来显示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fbb3a",
   "metadata": {},
   "source": [
    "## 2.7.3 Scrapy爬虫实例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991cddf",
   "metadata": {},
   "source": [
    "访问古诗文网站（https://so.gushiwen.cn/mingjus/）， 使用Scrapy框架编写爬虫程序，爬取每个名句及其完整古诗内容，并把爬取到的数据分别保存到文本文件和MySQL数据库中。本实例需要使用开发工具PyCharm (Community Edition)，请到PyCharm官网（https://www.jetbrains.com/pycharm/download/） 下载PyCharm安装文件并安装。\n",
    "本实例包括以下几个步骤：\n",
    "* 新建工程；\n",
    "* 编写代码文件items.py；\n",
    "* 编写爬虫文件；\n",
    "* 编写代码文件pipelines.py；\n",
    "* 编写代码文件settings.py；\n",
    "* 运行程序；\n",
    "把数据保存到数据库中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b51711",
   "metadata": {},
   "source": [
    "## 1.新建工程 \n",
    "在PyCharm中新建一个名称为“scrapyProject”的工程。在“scrapyProject”工程底部打开Terminal窗口（如图3-8所示），在命令提示符后面输入命令“pip install scrapy”，下载Scrapy框架所需文件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1260433",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_3_1.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf72daa",
   "metadata": {},
   "source": [
    "下载完成后，继续输入命令“scrapy startproject poemScrapy”，创建Scrapy爬虫框架相关目录和文件。创建完成以后的具体目录结构如图所示，这些目录和文件都是由Scrapy框架自动创建的，不需要手动创建。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e3bbe",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_3_2.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f17be",
   "metadata": {},
   "source": [
    "在Scrapy爬虫程序目录结构中，各个目录和文件的作用如下：  \n",
    "* Spiders目录：该目录下包含爬虫文件，需编码实现爬虫过程；  \n",
    "* __init__.py：为Python模块初始化目录，可以什么都不写，但是必须要有；  \n",
    "* items.py：模型文件，存放了需要爬取的字段；  \n",
    "* middlewares.py：中间件（爬虫中间件、下载中间件），本案例中不用此文件；  \n",
    "* pipelines.py：管道文件，用于配置数据持久化，例如写入数据库；  \n",
    "* settings.py：爬虫配置文件；  \n",
    "scrapy.cfg：项目基础设置文件，设置爬虫启用功能等。在本案例中不用此文件。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5e3a9",
   "metadata": {},
   "source": [
    "## 2.编写代码文件items.py   \n",
    "在items.py中定义字段用于保存数据，items.py的具体代码内容如下：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df69eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    " \n",
    "class PoemscrapyItem(scrapy.Item):\n",
    "    # 名句\n",
    "    sentence = scrapy.Field()\n",
    "    # 出处\n",
    "    source = scrapy.Field()\n",
    "    # 全文链接\n",
    "    url = scrapy.Field()\n",
    "    # 名句详细信息\n",
    "    content = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c55108",
   "metadata": {},
   "source": [
    "## 3.编写爬虫文件\n",
    "在Terminal窗口输入命令“cd poemScrapy”，进入对应的爬虫工程中，再输入命令“scrapy genspider poemSpider gushiwen.cn”，这时，在spiders目录下会出现一个新的Python文件poemSpider.py，该文件就是我们要编写爬虫程序的位置。下面是poemSpider.py的具体代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f472ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Request\n",
    "from ..items import PoemscrapyItem\n",
    " \n",
    "class PoemspiderSpider(scrapy.Spider):\n",
    "    name = 'poemSpider'   # 用于区别不同的爬虫\n",
    "    allowed_domains = ['gushiwen.cn']  # 允许访问的域\n",
    "    start_urls = ['http://so.gushiwen.cn/mingjus/']  # 爬取的地址\n",
    " \n",
    "    def parse(self, response):\n",
    "        # 先获每句名句的div\n",
    "        for box in response.xpath('//*[@id=\"html\"]/body/div[2]/div[1]/div[2]/div'):\n",
    "            # 获取每句名句的链接\n",
    "            url = 'https://so.gushiwen.cn' + box.xpath('.//@href').get()\n",
    "            # 获取每句名句内容\n",
    "            sentence = box.xpath('.//a[1]/text()') .get()\n",
    "            # 获取每句名句出处\n",
    "            source = box.xpath('.//a[2]/text()') .get()\n",
    "            # 实例化容器\n",
    "            item = PoemscrapyItem()\n",
    "            # # 将收集到的信息封装起来\n",
    "            item['url'] = url\n",
    "            item['sentence'] = sentence\n",
    "            item['source'] = source\n",
    "            # 处理子页\n",
    "            yield scrapy.Request(url=url, meta={'item': item}, callback=self.parse_detail)\n",
    "        # 翻页\n",
    "        next = response.xpath('//a[@class=\"amore\"]/@href'). get()\n",
    "        if next is not None:\n",
    "            next_url = 'https://so.gushiwen.cn' + next\n",
    "            # 处理下一页内容\n",
    "            yield Request(next_url)\n",
    "    def parse_detail(self, response):\n",
    "        # 获取名句的详细信息\n",
    "        item = response.meta['item']\n",
    "        content_list = response.xpath('//div[@class=\"contson\"]//text()').getall()\n",
    "        content = \"\".join(content_list).strip().replace('\\n', '').replace('\\u3000', '')\n",
    "        item['content'] = content\n",
    "        yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff8e8b",
   "metadata": {},
   "source": [
    "在上面的代码中，response.xpath()返回的是scrapy.selector.unified.SelectorList对象，比如response.xpath('//div[@class=\"contson\"]//text()')返回的部分结果如下：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a0edf",
   "metadata": {},
   "source": [
    "```\n",
    "[<Selector xpath='//div[@class=\"contson\"]//text()' data='\\n日日望乡国，空歌白苎词。'>, <Selector xpath='//div[@class=\"contson\"]//text()' data='长因送人处，忆得别家时。'>, <Selector xpath='//div[@class=\"contson\"]//text()' data='失意还独语，多愁只自知。'>, <Selector xpath='//div[@class=\"contson\"]//text()' data='客亭门外柳，折尽向南枝。\\n'>]\n",
    "![image.png](attachment:image.png)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d12ef",
   "metadata": {},
   "source": [
    "这时，response.xpath('//div[@class=\"contson\"]//text()').get()返回的结果如下：\n",
    "注意，这里会输出一个空行  \n",
    "> '日日望乡国，空歌白苎词。'\n",
    "response.xpath('//div[@class=\"contson\"]//text()').getall()返回的结果如下：\n",
    "> ['\\n日日望乡国，空歌白苎词。', '长因送人处，忆得别家时。', '失意还独语，多愁只自知。', '客亭门外柳，折尽向南枝。\\n']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62407432",
   "metadata": {},
   "source": [
    "## 4.编写代码文件pipelines.py\n",
    "\n",
    "当我们成功获取需要的信息后，要对信息进行存储。在Scrapy爬虫框架中，当item被爬虫收集完后，将会被传递到pipelines。现在要将爬取到的数据保存到文本文件中，可以使用的pipelines.py代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012b8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "class PoemscrapyPipeline:\n",
    "    def __init__(self):\n",
    "        # 打开文件\n",
    "        self.file = open('data.txt', 'w', encoding='utf-8')\n",
    " \n",
    "    def process_item(self, item, spider):\n",
    "        # 读取item中的数据\n",
    "        line = json.dumps(dict(item), ensure_ascii=False) + '\\n'\n",
    "        # 写入文件\n",
    "        self.file.write(line)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38391e",
   "metadata": {},
   "source": [
    "## 5.编写代码文件settings.py\n",
    "settings.py的具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2442ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT_NAME = 'poemScrapy'\n",
    " \n",
    "SPIDER_MODULES = ['poemScrapy.spiders']\n",
    "NEWSPIDER_MODULE = 'poemScrapy.spiders'\n",
    " \n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4421.5 Safari/537.36'\n",
    " \n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    " \n",
    "# 设置日志打印的等级\n",
    "LOG_LEVEL = 'WARNING'\n",
    " \n",
    "ITEM_PIPELINES = {\n",
    "    'poemScrapy.pipelines.PoemscrapyPipeline': 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394e0fc",
   "metadata": {},
   "source": [
    "其中，更改USER-AGENT和ROBOTSTXT_OBEY是为了避免访问被拦截或出错；设置LOG_LEVEL是为了避免在爬取过程中显示过多的日志信息；设置ITEM_PIPELINES是因为本案例使用到pipeline，需要先注册pipeline，右侧的数字‘1’为该pipeline的优先级，范围1-1000，数值越小越优先执行。读者也可以根据实际需求，适当更改settings.py中的内容。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c28597a",
   "metadata": {},
   "source": [
    "## 6.运行程序\n",
    "有两种执行Scrapy爬虫的方法，第一种是在Terminal窗口中输入命令“scrapy crawl poemSpider”，然后回车运行，等待几秒钟后即可完成数据的爬取。第二种是在poemScrapy目录下新建Python文件run.py（run.py应与scrapy.cfg文件在同一层目录下），并输入下面代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import cmdline\n",
    "cmdline.execute(\"scrapy crawl poemSpider\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d1ec8",
   "metadata": {},
   "source": [
    "在run.py代码区域点击鼠标右键，在弹出的菜单里选择“Run”运行代码，就可以执行Scrapy爬虫程序。执行成功以后，就可以看到生成的数据文件data.txt，其内容类似如下：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b2903",
   "metadata": {},
   "source": [
    "```\n",
    "{\"url\": \"https://so.gushiwen.cn/mingju/juv_2f9cf2c444f2.aspx\", \"sentence\": \"人道恶盈而好谦。\", \"source\": \"《易传·彖传上·谦》\", \"content\": \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbec6c1",
   "metadata": {},
   "source": [
    "## 7．把数据保存到数据库中\n",
    "为了把爬取到的数据保存到MySQL数据库中，需要首先安装PyMySQL模块。\n",
    "在PyCharm开发界面中点击“File->Settings…”，在打开的设置界面中（如图3-10所示），先点击“Project scrapyProject”，再点击“Python Interpreter”，会弹出如图所示的设置界面，点击界面底部的“+”。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07823b4a",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_3_3.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ccc59",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_3_4.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb281d",
   "metadata": {},
   "source": [
    "在弹出的模块安装界面中（如图3-12所示），先在搜索框中输入“pymysql”，然后，在搜索到的结果中点击“PyMySQL”条目，最后，点击界面底部的“Install Package”，开始安装模块，如果安装成功，会出现如图所示的信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87040c3",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_3_5.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067db3b",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_3_6.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c2f67",
   "metadata": {},
   "source": [
    "在Windows系统中启动MySQL服务进程，然后，打开MySQL命令行客户端，执行如下SQL语句创建一个名称为“poem”的数据库：  \n",
    "``CREATE DATABASE poem;  ``  \n",
    "然后，在poem数据库中创建一个名称为“beautifulsentence”的表，具体SQL语句如下：  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d4ff4",
   "metadata": {},
   "source": [
    "```\n",
    "DROP TABLE IF EXISTS `beautifulsentence`;\n",
    "CREATE TABLE `beautifulsentence` (\n",
    "  `source` varchar(255) NOT NULL,\n",
    "  `sentence` varchar(255) NOT NULL,\n",
    "  `content` text NOT NULL,\n",
    "  `url` varchar(255) NOT NULL\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8;\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddf794",
   "metadata": {},
   "source": [
    "修改pipelines.py，编写完成以后的pipelines.py代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "631f3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itemadapter import ItemAdapter\n",
    "import json\n",
    "import pymysql\n",
    " \n",
    "class PoemscrapyPipeline:\n",
    "    def __init__(self):\n",
    "        # 连接MySQL数据库\n",
    "        self.connect = pymysql.connect(\n",
    "            host='localhost',\n",
    "            port=3306,\n",
    "            user='root',\n",
    "            passwd='123456',  #设置成用户自己的数据库密码\n",
    "            db='poem',\n",
    "            charset='utf8'\n",
    "        )\n",
    "        self.cursor = self.connect.cursor()\n",
    " \n",
    "    def process_item(self, item, spider):\n",
    "        # 写入数据库\n",
    "        self.cursor.execute('INSERT INTO beautifulsentence(source,sentence,content,url) VALUES (\"{}\",\"{}\",\"{}\",\"{}\")'.format(item['source'], item['sentence'], item['content'], item['url']))\n",
    "        self.connect.commit()\n",
    "        return item\n",
    "def close_spider(self, spider):\n",
    "    # 关闭数据库连接\n",
    "        self.cursor.close()\n",
    "        self.connect.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd9d30",
   "metadata": {},
   "source": [
    "执行Scrapy爬虫程序，执行结束以后，如果执行成功，可以到MySQL数据库中使用如下命令查看数据：  \n",
    "USE poem;  \n",
    "SELECT * FROM beautifulsentence;  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5e4d8",
   "metadata": {},
   "source": [
    "网络爬虫系统的功能是下载网页数据，为搜索引擎系统或需要网络数据的企业提供数据来源。本章内容介绍了网络爬虫程序的编写方法，主要包括如何请求网页以及如何解析网页。在网页请求环节，需要注意的是，一些网站设置了反爬机制，会导致我们爬取网页失败。在网页解析环节，我们可以灵活运用BeautifulSoup提供的各种方法获取我们需要的数据。同时，为了减少程序开发工作量，可以选择包括Scrapy在内的一些网络爬虫开发框架编写网络网络爬虫程序。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744ddd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
