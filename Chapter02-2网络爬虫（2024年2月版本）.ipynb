{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44007ece",
   "metadata": {},
   "source": [
    "<center><h1>网络爬虫</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a65ff",
   "metadata": {},
   "source": [
    "* 2.1网络爬虫概述\n",
    "* 2.2网页基础知识\n",
    "* 2.3用Python实现HTTP请求\n",
    "* 2.4定制requests\n",
    "* 2.5解析网页\n",
    "* 2.6综合实例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6811a3",
   "metadata": {},
   "source": [
    "# 2.1 网络爬虫概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93113d71",
   "metadata": {},
   "source": [
    "## 2.1.1 什么是网络爬虫\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db2234",
   "metadata": {},
   "source": [
    "网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成部分。如图所示，爬虫从一个或若干个初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083a15c",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_2_11.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8245de7",
   "metadata": {},
   "source": [
    "## 2.1.2网络爬虫的类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191b74d",
   "metadata": {},
   "source": [
    "网络爬虫的类型可以分为：\n",
    "* 通用网络爬虫\n",
    "* 聚焦网络爬虫\n",
    "* 增量式网络爬虫\n",
    "* 深层网络爬虫\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c9f32",
   "metadata": {},
   "source": [
    "## 2.1.3反爬机制\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42d0fd",
   "metadata": {},
   "source": [
    "为什么会有反爬机制？原因主要有两点：  \n",
    "* 第一，在大数据时代，数据是十分宝贵的财富，很多企业不愿意让自己的数据被别人免费获取，因此，很多企业都为自己的网站运用了反爬机制，防止网页上的数据被爬走；  \n",
    "* 第二，简单低级的网络爬虫，数据采集速度快，伪装度低，如果没有反爬机制，它们可以很快地抓取大量数据，甚至因为请求过多，造成网站服务器不能正常工作，影响了企业的业务开展。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afe66f",
   "metadata": {},
   "source": [
    "反爬机制也是一把双刃剑，一方面可以保护企业网站和网站数据，但是，另一方面，如果反爬机制过于严格，可能会误伤到真正的用户请求，也就是真正用户的请求被错误当成网络爬虫而被拒绝访问。如果既要和“网络爬虫”死磕，又要保证很低的误伤率，那么又会增加网站研发的成本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aae8d3",
   "metadata": {},
   "source": [
    "# 2.2 网页基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a30fa8",
   "metadata": {},
   "source": [
    "## 2.2.1 超文本和HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9230b",
   "metadata": {},
   "source": [
    "超文本（Hypertext）是指使用超链接的方法，把文字和图片信息相互联结，形成具有相关信息的体系。超文本的格式有很多，目前最常使用的是超文本标记语言HTML（Hyper Text Markup Language），我们平时在浏览器里面看到的网页就是由HTML解析而成的。下面是网页文件web_demo.html的HTML源代码：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1f419",
   "metadata": {},
   "source": [
    "```html\n",
    "<html><head><title>搜索指数</title></head>\n",
    "<body>\n",
    "<table>\n",
    "<tr><td>排名</td><td>关键词</td><td>搜索指数</td></tr>\n",
    "<tr><td>1</td><td>大数据</td><td>187767</td></tr>\n",
    "<tr><td>2</td><td>云计算</td><td>178856</td></tr>\n",
    "<tr><td>3</td><td>物联网</td><td>122376</td></tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e3608",
   "metadata": {},
   "source": [
    "使用网页浏览器（比如IE、Firefox等）打开这个网页文件，就会看到如图所示的网页内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e206e5",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_2_12.png\"  >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b3a56",
   "metadata": {},
   "source": [
    "## 2.2.2 HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d048c",
   "metadata": {},
   "source": [
    "HTTP是由万维网协会（World Wide Web Consortium）和 Internet 工作小组IETF（Internet Engineering Task Force）共同制定的规范。HTTP的全称是“Hyper Text Transfer Protocol”，中文名叫做“超文本传输协议”。HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本内容。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec9a96",
   "metadata": {},
   "source": [
    "HTTP是基于“客户端/服务器”架构进行通信的，HTTP的服务器端实现程序有httpd、nginx等，客户端的实现程序主要是Web浏览器，例如Firefox、Internet Explorer、Google Chrome、Safari、Opera等。Web浏览器和Web服务器之间可以通过HTTP进行通信。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900bebb",
   "metadata": {},
   "source": [
    "一个典型的HTTP请求过程如下（如图所示）：\n",
    "    （1）用户在浏览器中输入网址，比如http://www.cucn.edu.cn， 浏览器向网页服务器发起请求；\n",
    "    （2）网页服务器接收用户访问请求，处理请求，产生响应（即把处理结果以HTML形式返回给浏览器）；\n",
    "    （3）浏览器接收来自网页服务器的HTML内容，进行渲染以后展示给用户。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b96bab",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_2_13.png\" width=\"600\" height=\"600\" >   \n",
    "<center>一个典型的HTTP请求过程</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d150de",
   "metadata": {},
   "source": [
    "# 2.3 用Python实现HTTP请求\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64419618",
   "metadata": {},
   "source": [
    "* 2.3.1 urllib模块\n",
    "* 2.3.2 urllib3模块\n",
    "* 2.3.3 requests模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369862be",
   "metadata": {},
   "source": [
    "## 2.3.1urllib模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053a41a",
   "metadata": {},
   "source": [
    "urllib是Python自带模块，该模块提供了一个urlopen()方法，通过该方法指定URL发送HTTP请求来获取数据。urllib提供了多个子模块，具体的模块名称与功能如表所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b07b4",
   "metadata": {},
   "source": [
    "|  模块名称   | 功能 |\n",
    "|  :----  | :----  |\n",
    "| urllib.request  | 该模块定义了打开URL（主要是HTTP）的方法和类，如身份验证、重定向和cookie等 |\n",
    "| urllib.error | 该模块中主要包含异常类，基本的异常类是URLError |\n",
    "| urllib.parse  | 该模块定义的功能分为两大类：URL解析和URL引用 |\n",
    "| urllib.robotparser  | 该模块用于解析robots.txt文件 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f33ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是通过urllib.request模块实现发送GET请求获取网页内容的实例：\n",
    "import urllib.request\n",
    "response=urllib.request.urlopen(\"http://www.baidu.com\")\n",
    "html=response.read()\n",
    "print(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24353d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"errno\":0,\"data\":[{\"k\":\"\\u82f9\\u679c\",\"v\":\"\\u540d. apple\"},{\"k\":\"\\u82f9\\u679c\\u56ed\",\"v\":\"apple grove\"},{\"k\":\"\\u82f9\\u679c\\u5934\",\"v\":\"apple head\"},{\"k\":\"\\u82f9\\u679c\\u5e72\",\"v\":\"[\\u533b]dried apple\"},{\"k\":\"\\u82f9\\u679c\\u6728\",\"v\":\"applewood\"}],\"logid\":350782259}\n"
     ]
    }
   ],
   "source": [
    "# 下面是通过urllib.request模块实现发送POST请求获取网页内容的实例：\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "# 1.指定url\n",
    "url = 'https://fanyi.baidu.com/sug'\n",
    "# 2.发起POST请求之前，要处理POST请求携带的参数\n",
    "# 2.1 将POST请求封装到字典\n",
    "data = {'kw':'苹果',}\n",
    "# 2.2 使用parse模块中的urlencode(返回值类型是字符串类型)进行编码处理\n",
    "data = urllib.parse.urlencode(data)\n",
    "# 将步骤2.2的编码结果转换成byte类型\n",
    "data = data.encode()\n",
    "# 3.发起POST请求:urlopen函数的data参数表示的就是经过处理之后的POST请求携带的参数\n",
    "response = urllib.request.urlopen(url=url,data=data)\n",
    "data = response.read()\n",
    "# print(data)\n",
    "s = data.decode('utf-8')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab7db4",
   "metadata": {},
   "source": [
    "把上面print(data)执行的结果，拿到JSON在线格式校验网站 http://www.bejson.com 进行处理，使用“Unicode转中文”功能可以得到如下结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5dd6c0",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b240fa",
   "metadata": {},
   "source": [
    "## 2.3.2urllib3模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83518ab",
   "metadata": {},
   "source": [
    "urllib3是一个功能强大、条理清晰、用于HTTP客户端的Python库，许多Python的原生系统已经开始使用urllib3。urllib3提供了很多python标准库里所没有的重要特性，包括：线程安全、连接池、客户端SSL/TLS验证、文件分部编码上传、协助处理重复请求和HTTP重定位、支持压缩编码、支持HTTP和SOCKS代理、100%测试覆盖率等。\n",
    "在使用urllib3之前，需要打开一个cmd窗口使用如下命令进行安装：pip install urllib3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是通过GET请求获取网页内容的实例：\n",
    "import urllib3\n",
    "#需要一个PoolManager实例来生成请求，由该实例对象处理与线程池的连接以及线程安全的所有细节，不需要任何人为操作\n",
    "http = urllib3.PoolManager()\n",
    "response = http.request('GET','http://www.baidu.com')\n",
    "print(response.status)\n",
    "print(response.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a80351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是通过POST请求获取网页内容的实例：\n",
    "import urllib3\n",
    "http = urllib3.PoolManager()\n",
    "response = http.request('POST','https://fanyi.baidu.com/sug',fields={'kw':'苹果'})\n",
    "print(response.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef909ef",
   "metadata": {},
   "source": [
    "把上面print(data)执行的结果，拿到JSON在线格式校验网站 http://www.bejson.com 进行处理，使用“Unicode转中文”功能可以得到如下结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2a1e0",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4850f1",
   "metadata": {},
   "source": [
    "## 2.3.3requests模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da78c49",
   "metadata": {},
   "source": [
    "requests库是一个非常好用的HTTP请求库，可用于网络请求和网络爬虫等。在使用requests之前，需要打开一个cmd窗口使用如下命令进行安装：\n",
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de17c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态码: 200\n",
      "--------------------\n",
      "url: http://www.baidu.com/\n",
      "--------------------\n",
      "header: {'Connection': 'close', 'Transfer-Encoding': 'chunked', 'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Thu, 14 Mar 2024 10:00:16 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:27:56 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/'}\n",
      "--------------------\n",
      "cookie: <RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>\n",
      "--------------------\n",
      "text: <!DOCTYPE html>\r\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=ç¾åº¦ä¸ä¸ class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ°é»</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å°å¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§é¢</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç»å½</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">ç»å½</a>');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">æ´å¤äº§å</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å",
      "³äºç¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç¨ç¾åº¦åå¿",
      "è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æè§åé¦</a>&nbsp;äº¬ICPè¯030173å·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n",
      "\n",
      "--------------------\n",
      "content: b'<!DOCTYPE html>\\r\\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>\\xe7\\x99\\xbe\\xe5\\xba\\xa6\\xe4\\xb8\\x80\\xe4\\xb8\\x8b\\xef\\xbc\\x8c\\xe4\\xbd\\xa0\\xe5\\xb0\\xb1\\xe7\\x9f\\xa5\\xe9\\x81\\x93</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=\\xe7\\x99\\xbe\\xe5\\xba\\xa6\\xe4\\xb8\\x80\\xe4\\xb8\\x8b class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>\\xe6\\x96\\xb0\\xe9\\x97\\xbb</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>\\xe5\\x9c\\xb0\\xe5\\x9b\\xbe</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>\\xe8\\xa7\\x86\\xe9\\xa2\\x91</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>\\xe8\\xb4\\xb4\\xe5\\x90\\xa7</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>\\xe7\\x99\\xbb\\xe5\\xbd\\x95</a> </noscript> <script>document.write(\\'<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\\'+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ \\'\" name=\"tj_login\" class=\"lb\">\\xe7\\x99\\xbb\\xe5\\xbd\\x95</a>\\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">\\xe6\\x9b\\xb4\\xe5\\xa4\\x9a\\xe4\\xba\\xa7\\xe5\\x93\\x81</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>\\xe5\\x85\\xb3\\xe4\\xba\\x8e\\xe7\\x99\\xbe\\xe5\\xba\\xa6</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>\\xe4\\xbd\\xbf\\xe7\\x94\\xa8\\xe7\\x99\\xbe\\xe5\\xba\\xa6\\xe5\\x89\\x8d\\xe5\\xbf\\x85\\xe8\\xaf\\xbb</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>\\xe6\\x84\\x8f\\xe8\\xa7\\x81\\xe5\\x8f\\x8d\\xe9\\xa6\\x88</a>&nbsp;\\xe4\\xba\\xacICP\\xe8\\xaf\\x81030173\\xe5\\x8f\\xb7&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 以GET请求方式为例，打印多种请求信息的代码如下：\n",
    "import requests\n",
    "response = requests.get('http://www.baidu.com')  #对需要爬取的网页发送请求\n",
    "print('状态码:',response.status_code)  #打印状态码\n",
    "print('-'*20)\n",
    "print('url:',response.url)  #打印请求url\n",
    "print('-'*20)\n",
    "print('header:',response.headers)  #打印头部信息\n",
    "print('-'*20)\n",
    "print('cookie:',response.cookies)  #打印cookie信息\n",
    "print('-'*20)\n",
    "print('text:',response.text)  #以文本形式打印网页源码\n",
    "print('-'*20)\n",
    "print('content:',response.content)  #以字节流形式打印网页源码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58316e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"errno\":0,\"data\":[{\"k\":\"\\\\u82f9\\\\u679c\",\"v\":\"\\\\u540d. apple\"},{\"k\":\"\\\\u82f9\\\\u679c\\\\u56ed\",\"v\":\"apple grove\"},{\"k\":\"\\\\u82f9\\\\u679c\\\\u5934\",\"v\":\"apple head\"},{\"k\":\"\\\\u82f9\\\\u679c\\\\u5e72\",\"v\":\"[\\\\u533b]dried apple\"},{\"k\":\"\\\\u82f9\\\\u679c\\\\u6728\",\"v\":\"applewood\"}],\"logid\":1498905899}'\n"
     ]
    }
   ],
   "source": [
    "# 以POST请求方式发送HTTP网页请求的示例代码如下：\n",
    "import requests\n",
    "#导入模块\n",
    "import requests\n",
    "#表单参数\n",
    "data = {'kw':'苹果',}\n",
    "#对需要爬取的网页发送请求\n",
    "response = requests.post('https://fanyi.baidu.com/sug',data=data)\n",
    "#以字节流形式打印网页源码\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185dcc50",
   "metadata": {},
   "source": [
    "# 2.4 定制requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6d4ef",
   "metadata": {},
   "source": [
    "* 2.4.1 传递URL参数\n",
    "* 2.4.2 定制请求头\n",
    "* 2.4.3 网络超时\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf3751",
   "metadata": {},
   "source": [
    "## 2.4.1传递URL参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55859aff",
   "metadata": {},
   "source": [
    "为了请求特定的数据，我们需要在URL（Uniform Resource Locator）的查询字符串中加入一些特定数据。这些数据一般会跟在一个问号后面，并且以键值对的形式放在URL中。在requests中，我们可以直接把这些参数保存在字典中，用params构建到URL中。具体实例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "base_url = 'http://httpbin.org'\n",
    "param_data = {'user':'xmu','password':'123456'}\n",
    "response = requests.get(base_url+'/get',params=param_data)\n",
    "print(response.url)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720e9ae",
   "metadata": {},
   "source": [
    "## 2.4.2 定制请求头"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f775c",
   "metadata": {},
   "source": [
    "  在爬取网页的时候，输出的信息中有时候会出现“抱歉，无法访问”等字眼，这就是禁止爬取，需要通过定制请求头Headers来解决这个问题。定制Headers是解决requests请求被拒绝的方法之一，相当于我们进入这个网页服务器，假装自己本身在爬取数据。请求头Headers提供了关于请求、响应或其他发送实体的消息，如果没有定制请求头或请求的请求头和实际网页不一致，就可能无法返回正确结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf82d7",
   "metadata": {},
   "source": [
    "  获取一个网页的Headers的方法如下：使用360、火狐或谷歌浏览器打开一个网址（比如http://httpbin.org/ , 在网页上单击鼠标右键，在弹出的菜单中选择“查看元素”，然后刷新网页，再按照如下图所示的步骤，先点击“Network”选项卡，再点击“Doc”，接下来点击“Name”下方的网址，就会出现类似如下的Headers信息：\n",
    "User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a152cb2",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a6097a",
   "metadata": {},
   "source": [
    "Headers中有很多内容，主要常用的就是“User-Agent”和“Host”，它们是以键对的形式呈现的，如果把“User-Agent”以字典键值对形式作为Headers的内容，往往就可以顺利爬取网页内容。\n",
    "下面是添加了Headers信息的网页请求过程：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='http://httpbin.org'\n",
    "# 创建头部信息\n",
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'}\n",
    "response = requests.get(url,headers=headers)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115f817",
   "metadata": {},
   "source": [
    "## 2.4.3 网络超时"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1cefe",
   "metadata": {},
   "source": [
    "网络请求不可避免会遇上请求超时的情况，这个时候，网络数据采集的程序会一直运行等待进程，造成网络数据采集程序不能很好地顺利执行。因此，可以为requests的timeout参数设定等待秒数，如果服务器在指定时间内没有应答就返回异常。具体代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42e7dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.exceptions import ReadTimeout,ConnectTimeout\n",
    "try:\n",
    "   response = requests.get(\"http://www.baidu.com\", timeout=0.5)\n",
    "   print(response.status_code)\n",
    "except ReadTimeout or ConnectTimeout:\n",
    "   print('Timeout')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee9185",
   "metadata": {},
   "source": [
    "# 2.5 解析网页\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285f12c",
   "metadata": {},
   "source": [
    "* 2.5.1 BeautifulSoup简介\n",
    "* 2.5.2 BeautifulSoup四大对象\n",
    "* 2.5.3 遍历文档树\n",
    "* 2.5.4 搜索文档树\n",
    "* 2.5.5 CSS选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3754ce",
   "metadata": {},
   "source": [
    "### 2.5.1 BeautifulSoup简介\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983560b1",
   "metadata": {},
   "source": [
    "BeautifulSoup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等。BeautifulSoup是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。BeautifulSoup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。BeautifulSoup3已经停止开发，目前推荐使用BeautifulSoup4，不过它已经被移植到bs4当中了，所以，在使用BeautifulSoup4之前，需要安装bs4：\n",
    "> pip install bs4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6dabf1",
   "metadata": {},
   "source": [
    "使用BeautifulSoup解析HTML比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持lxml的XML解析器和HTML解析器，此外还支持html5lib解析器，下表给出了每个解析器的优缺点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e55ec5",
   "metadata": {},
   "source": [
    "| 解析器 | 用法 | 优点 | 缺点 |\n",
    "|:-------|:------------------|:-------|:-------|\n",
    "|Python标准库 | BeautifulSoup(markup,\"html.parser\")  |Python标准库执行速度适中| 文档容错能力差|\n",
    "| lxml的HTML解析器 | BeautifulSoup(markup,\"lxml\") |速度快文档容错能力强|需要安装C语言库|\n",
    "| lxml的XML解析器 | BeautifulSoup(markup, \"lxml-xml\") BeautifulSoup(markup,\"xml\") |速度快唯一支持XML的解析器|需要安装C语言库|\n",
    "| html5lib | BeautifulSoup(markup, \"html5lib\") |兼容性好以浏览器的方式解析文档生成HTML5格式的文档|速度慢，不依赖外部扩展|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906bf11b",
   "metadata": {},
   "source": [
    "下面给出一个BeautifulSoup解析网页的简单实例，使用了lxml解析器，在使用之前，需要执行如下命令安装lxml解析器：\n",
    "> pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab7c8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   BigData Software\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    BigData Software\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"bigdata\">\n",
      "   There are three famous bigdata softwares; and their names are\n",
      "   <a class=\"software\" href=\"http://example.com/hadoop\" id=\"link1\">\n",
      "    Hadoop\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"software\" href=\"http://example.com/spark\" id=\"link2\">\n",
      "    Spark\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"software\" href=\"http://example.com/flink\" id=\"link3\">\n",
      "    Flink\n",
      "   </a>\n",
      "   ;\n",
      "            and they are widely used in real applications.\n",
      "  </p>\n",
      "  <p class=\"bigdata\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_doc = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>BigData Software</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"title\"><b>BigData Software</b></p>\n",
    "        <p class=\"bigdata\">There are three famous bigdata softwares; and their names are\n",
    "            <a href=\"http://example.com/hadoop\" class=\"software\" id=\"link1\">Hadoop</a>,\n",
    "            <a href=\"http://example.com/spark\" class=\"software\" id=\"link2\">Spark</a> and\n",
    "            <a href=\"http://example.com/flink\" class=\"software\" id=\"link3\">Flink</a>;\n",
    "            and they are widely used in real applications.\n",
    "        </p>\n",
    "        <p class=\"bigdata\">...</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc,\"lxml\")\n",
    "content = soup.prettify()\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2539b77",
   "metadata": {},
   "source": [
    "如果要更换解析器，比如要使用Python标准库的解析器，只需要把上面的“soup = BeautifulSoup(html_doc,\"lxml\")”这行代码替换成如下代码即可：\n",
    ">soup = BeautifulSoup(html_doc,\"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9c5ff",
   "metadata": {},
   "source": [
    "### 2.5.2 BeautifulSoup四大对象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e8704",
   "metadata": {},
   "source": [
    "BeautifulSoup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为4种：Tag、NavigableString、BeautifulSoup、Comment。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f875a8",
   "metadata": {},
   "source": [
    "<b>1.Tag</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898fc10",
   "metadata": {},
   "source": [
    "Tag就是HTML中的一个个标签，例如：\n",
    "```\n",
    "<title>BigData Software</title><a href=\"http://example.com/hadoop\" class=\"software\" id=\"link1\">Hadoop</a>\n",
    "```\n",
    "上面的<title>、<a>等标签加上里面包括的内容就是Tag，利用soup加标签名可以轻松地获取这些标签的内容。\n",
    "作为演示，我们可以继续执行以下代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8835c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `soup` not found.\n"
     ]
    }
   ],
   "source": [
    "soup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b9b741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"software\" href=\"http://example.com/hadoop\" id=\"link1\">Hadoop</a>\n",
      "<title>BigData Software</title>\n",
      "[document]\n",
      "{'class': ['title']}\n",
      "['title']\n",
      "['title']\n"
     ]
    }
   ],
   "source": [
    "print(soup.a)\n",
    "print(soup.title)\n",
    "# Tag有两个重要的属性，即name和attrs。下面继续执行如下代码：\n",
    "print(soup.name)\n",
    "print(soup.p.attrs)\n",
    "{'class': ['title']}\n",
    "# 如果想要单独获取某个属性，比如要获取“class”属性的值，可以执行如下代码：\n",
    "print(soup.p['class'])\n",
    "# 还可以利用get方法获得属性的值，代码如下：\n",
    "print(soup.p.get('class'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fed903",
   "metadata": {},
   "source": [
    "<b>2.NavigableString</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adaccf",
   "metadata": {},
   "source": [
    "NavigableString对象用于操纵字符串。在网页解析时，已经得到了标签的内容以后，如果我们想获取标签内部的文字，则可以使用.string方法，其返回值就是一个NavigableString对象，具体实例如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37564f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigData Software\n",
      "<class 'bs4.element.NavigableString'>\n"
     ]
    }
   ],
   "source": [
    "print(soup.p.string)\n",
    "print(type(soup.p.string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d07805",
   "metadata": {},
   "source": [
    "<b>3.BeautifulSoup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1652f07",
   "metadata": {},
   "source": [
    "BeautifulSoup对象表示的是一个文档的全部内容，大部分时候，可以把它当作Tag对象，是一个特殊的Tag。例如，可以分别获取它的类型、名称以及属性!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3251962d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "[document]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(type(soup.name))\n",
    "print(soup.name)\n",
    "print(soup.attrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257d2ad",
   "metadata": {},
   "source": [
    "<b>4.Comment</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaaefd5",
   "metadata": {},
   "source": [
    "Comment对象是一种特殊类型的NavigableString对象，输出的内容不包括注释符号。如果它处理不好，可能会对文本处理造成意想不到的麻烦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0e70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>\n",
      " Elsie \n",
      "<class 'bs4.element.Comment'>\n"
     ]
    }
   ],
   "source": [
    "html_doc = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>The Dormouse's story</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "        <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"><!-- Elsie --></a>,\n",
    "            <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "            <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "            and they lived at the bottom of a well.\n",
    "        </p>\n",
    "        <p class=\"story\">...</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc,\"lxml\")\n",
    "print(soup.a)\n",
    "print(soup.a.string)\n",
    "print(type(soup.a.string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2b3a2",
   "metadata": {},
   "source": [
    "从上面执行结果可以看出，a标签里的内容“<!-- Elsie -->”实际上是注释，但是使用语句print(soup.a.string)输出它的内容以后会发现，它已经把注释符号去掉了，只输出了“Elsie”，所以这可能会给我们带来不必要的麻烦。另外我们打印输出它的类型，发现它是一个Comment类型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8b6b7",
   "metadata": {},
   "source": [
    "### 2.5.3 遍历文档树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0437dc",
   "metadata": {},
   "source": [
    "遍历文档树就是从根节点html标签开始遍历，直到找到目标元素为止。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692c6cd",
   "metadata": {},
   "source": [
    "#### 1.直接子节点  \n",
    "（1）.contents属性  \n",
    "Tag对象的.contents属性可以将某个Tag的子节点以列表的方式输出，当然列表会允许用索引的方式来获取列表中的元素。下面是示例代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeffde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>BigData Software</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"title\"><b>BigData Software</b></p>\n",
    "        <p class=\"bigdata\">There are three famous bigdata softwares; and their names are\n",
    "            <a href=\"http://example.com/hadoop\" class=\"software\" id=\"link1\">Hadoop</a>,\n",
    "            <a href=\"http://example.com/spark\" class=\"software\" id=\"link2\">Spark</a> and\n",
    "            <a href=\"http://example.com/flink\" class=\"software\" id=\"link3\">Flink</a>;\n",
    "            and they are widely used in real applications.\n",
    "        </p>\n",
    "        <p class=\"bigdata\">...</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc,\"lxml\")\n",
    "print(soup.body.contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用索引的方式来获取列表中的元素：\n",
    "print(soup.body.contents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fc279",
   "metadata": {},
   "source": [
    "（2）.children属性  \n",
    "Tag对象的.children属性是一个迭代器，可以使用for循环进行遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in soup.body.children:\n",
    "      print(child)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca7879",
   "metadata": {},
   "source": [
    "#### 2.所有子孙节点  \n",
    "在获取所有子孙节点时，可以使用.descendants属性，与Tag对象的.children和.contents仅包含Tag对象的直接子节点不同，该属性是将Tag对象的所有子孙结点进行递归循环，然后生成生成器。示例代码如下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in soup.descendants:\n",
    "    print(child)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ed7be",
   "metadata": {},
   "source": [
    "#### 3.节点内容\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdedd14",
   "metadata": {},
   "source": [
    "（1）Tag对象内没有标签的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64148bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ab4bd",
   "metadata": {},
   "source": [
    "（2）Tag对象内有一个标签的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894017e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.head)\n",
    "print(soup.head.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc8f18",
   "metadata": {},
   "source": [
    "（3）Tag对象内有多个标签的情况。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e5335",
   "metadata": {},
   "source": [
    "从上面的执行结果中可以看出，body标签内包含了多个p标签，这时如果使用.string获取子节点内容，就会返回None，代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.body.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d7f04",
   "metadata": {},
   "source": [
    "也就是说，如果Tag包含了多个子节点，Tag就无法确定.string应该调用哪个子节点的内容，因此.string的输出结果是None。这时应该使用.strings属性或.stripped_strings属性，它们获得的都是一个生成器，示例代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33209187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86268e4",
   "metadata": {},
   "source": [
    "可以用for循环对生成器进行遍历，代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e109c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in soup.strings:\n",
    "      print(repr(string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2923d69",
   "metadata": {},
   "source": [
    "使用Tag对象的.stripped_strings属性，可以获得去掉空白行的标签内的众多内容，示例代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742340a",
   "metadata": {},
   "outputs": [],
   "source": [
    " for string in soup.stripped_strings:\n",
    "          print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb7d46",
   "metadata": {},
   "source": [
    "#### 4.直接父节点\n",
    "使用Tag对象的.parent属性可以获得父节点，使用Tag对象的.parents属性可以获得从父到根的所有节点。\n",
    "下面是标签的父节点：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = soup.p\n",
    "print(p.parent.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9268a",
   "metadata": {},
   "source": [
    "下面是内容的父节点：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a225d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.head.title.string\n",
    "print(content)\n",
    "print(content.parent.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177af15",
   "metadata": {},
   "source": [
    "Tag对象的.parents属性，得到的也是一个生成器：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.head.title.string\n",
    "print(content)\n",
    "for parent in content.parents:\n",
    "      print(parent.name)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb23716",
   "metadata": {},
   "source": [
    "#### 5.兄弟节点\n",
    "可以使用Tag对象的.next_sibling和.previous_sibling属性分别获取下一个兄弟结点和获取上一个兄弟结点。需要注意的是，实际文档中Tag的.next_sibling和.previous_sibling属性通常是字符串或空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行。示例代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98114612",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.p.next_sibling)\n",
    "# 此处返回为空白\n",
    "print(soup.p.prev_sibling)\n",
    "#没有前一个兄弟节点，返回None\n",
    "print(soup.p.next_sibling.next_sibling)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad79ce2",
   "metadata": {},
   "source": [
    "```\n",
    "上面这个语句的返回结果如下：\n",
    "<p class=\"bigdata\">There are three famous bigdata softwares; and their names are\n",
    "<a class=\"software\" href=\"http://example.com/hadoop\" id=\"link1\">Hadoop</a>,\n",
    "<a class=\"software\" href=\"http://example.com/spark\" id=\"link2\">Spark</a> and\n",
    "<a class=\"software\" href=\"http://example.com/flink\" id=\"link3\">Flink</a>;\n",
    "and they are widely used in real applications.</p>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10402760",
   "metadata": {},
   "source": [
    "#### 6.全部兄弟节点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用Tag对象的.next_siblings和.previous_siblings属性对当前的兄弟结点迭代输出。示例代码如下：\n",
    "for next in soup.a.next_siblings:\n",
    "      print(repr(next))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e861824",
   "metadata": {},
   "source": [
    "#### 7.前后节点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafe488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag对象的.next_element和.previous_element 属性，用于获得不分层次的前后元素，示例代码如下：\n",
    "print(soup.a.previous_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b5ab8",
   "metadata": {},
   "source": [
    "#### 8. 所有前后节点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Tag对象的.next_elements和.previous_elements属性可以向前或向后解析文档内容，示例代码如下：\n",
    "for element in soup.a.next_elements:\n",
    "      print(repr(element))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9fcda1",
   "metadata": {},
   "source": [
    "### 2.5.4 搜索文档树\n",
    "搜索文档树是通过指定标签名来搜索元素，另外还可以通过指定标签的属性值来精确定位某个节点元素，最常用的两个方法就是find()和find_all()，这两个方法在BeatifulSoup和Tag对象上都可以被调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3dfad",
   "metadata": {},
   "source": [
    "#### 1.find_all()  \n",
    "find_all()方法搜索当前Tag的所有Tag子节点，并判断是否符合过滤器的条件，它的函数原型是：  \n",
    "find_all( name , attrs , recursive , text , **kwargs )  \n",
    "find_all()的返回值是一个Tag组成的列表，方法调用非常灵活，所有的参数都是可选的。  \n",
    "\n",
    "<b>（1）name参数 </b>   \n",
    "    name参数可以查找所有名字为name的Tag，字符串对象会被自动忽略掉。\n",
    "    \n",
    "    \n",
    " ①传入字符串  \n",
    "    查找所有名字为a的Tag，代码如下：  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a05543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all('a'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b27594",
   "metadata": {},
   "source": [
    "②传入正则表达式\n",
    "如果传入正则表达式作为参数，BeautifulSoup会通过正则表达式的match()来匹配内容.下面例子中找出所有以b开头的标签，这表示`<body>`和`<b>`标签都应该被找到：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6359ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for tag in soup.find_all(re.compile(\"^b\")):\n",
    "  print(tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb277c1",
   "metadata": {},
   "source": [
    "③传入列表\n",
    "如果传入参数是列表，BeautifulSoup会将与列表中任一元素匹配的内容返回。下面代码找到文档中所有`<a>`标签和`<b>`标签：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfdff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all([\"a\",\"b\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90982cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "④传入True\n",
    "传入True可以找到所有的标签。下面的例子在文档树中查找所有包含id属性的标签，无论id的值是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all(id=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a2c22",
   "metadata": {},
   "source": [
    "⑤传入方法  \n",
    "如果没有合适过滤器，那么还可以定义一个方法，方法只接受一个元素参数，如果这个方法返回True，表示当前元素匹配并且被找到，如果不是则返回False。下面方法对当前元素进行校验，如果包含class属性却不包含id属性，那么将返回True：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_class_but_no_id(tag): \n",
    "    return tag.has_attr('class') and not tag.has_attr('id')\n",
    "# 将这个方法作为参数传入find_all()方法，将得到所有<p>标签：\n",
    "print(soup.find_all(has_class_but_no_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd667370",
   "metadata": {},
   "source": [
    "<b>（2）keyword参数</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过name参数是搜索Tag的标签类型名称，如a、head、title等。如果要通过标签内属性的值来搜索，要通过键值对的形式来指定，实例如下：\n",
    "import re\n",
    "print(soup.find_all(id='link2'))\n",
    "\n",
    "print(soup.find_all(href=re.compile(\"spark\")))\n",
    "\n",
    "# 使用多个指定名字的参数可以同时过滤Tag的多个属性：\n",
    "soup.find_all(href=re.compile(\"hadoop\"), id='link1')\n",
    "\n",
    "# 如果指定的key是Python的关键词，则后面需要加下划线：\n",
    "print(soup.find_all(class_=\"software\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db798ac1",
   "metadata": {},
   "source": [
    "<b>（3）text参数</b>  \n",
    "text参数的作用和name参数类似，但是text参数的搜索范围是文档中的字符串内容（不包含注释），并且是完全匹配，当然也接受正则表达式、列表、True。实例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f2983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "print(soup.a)\n",
    "\n",
    "print(soup.find_all(text=\"Hadoop\"))\n",
    "\n",
    "print(soup.find_all(text=[\"Hadoop\", \"Spark\", \"Flink\"]))\n",
    "\n",
    "print(soup.find_all(text=\"bigdata\"))\n",
    "\n",
    "print(soup.find_all(text=\"BigData Software\"))\n",
    "\n",
    "print(soup.find_all(text=re.compile(\"bigdata\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56c6af",
   "metadata": {},
   "source": [
    "<b>（4）limit参数</b>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404aa6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以通过limit参数来限制使用name参数或者attrs参数过滤出来的条目的数量，实例如下：\n",
    "print(soup.find_all(\"a\"))\n",
    "\n",
    "print(soup.find_all(\"a\",limit=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7a326",
   "metadata": {},
   "source": [
    "<b>（5）recursive 参数</b>  \n",
    "调用Tag的find_all()方法时，BeautifulSoup会检索当前Tag的所有子孙节点，如果只想搜索Tag的直接子节点，可以使用参数recursive=False，实例如下  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cefe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.body.find_all(\"a\",recursive=False))\n",
    "\n",
    "# 在这个例子中，a标签都是在p标签内的，所以在body的直接子节点下搜索a标签是无法匹配到a标签的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66165a3",
   "metadata": {},
   "source": [
    "#### 2.find()  \n",
    "\n",
    "find()与find_all()的区别是，find_all()将所有匹配的条目组合成一个列表，而find()仅返回第一个匹配的条目，除此以外，二者的用法都相同!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b996b",
   "metadata": {},
   "source": [
    "### 2.5.5 CSS选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af907ee",
   "metadata": {},
   "source": [
    "BeautifulSoup支持大部分的CSS选择器，在Tag或BeautifulSoup对象的select()方法中传入字符串参数，即可使用CSS选择器的语法找到标签。 \n",
    "\n",
    "#### ①通过标签名查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('title'))\n",
    "print(soup.select('a'))\n",
    "print(soup.select('b'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1df2d",
   "metadata": {},
   "source": [
    "#### ②通过类名查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ea118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('.software'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6db82",
   "metadata": {},
   "source": [
    "#### ③通过id名查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('#link1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b997f07",
   "metadata": {},
   "source": [
    "#### ④组合查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('p #link1'))\n",
    "\n",
    "print(soup.select(\"head > title\"))\n",
    "\n",
    "print(soup.select(\"p > a:nth-of-type(1)\"))\n",
    "\n",
    "print(soup.select(\"p > a:nth-of-type(2)\"))\n",
    "\n",
    "print(soup.select(\"p > a:nth-of-type(3)\"))\n",
    "\n",
    "# 在上面的语句中，\"p > a:nth-of-type(2)\"的含义是：p元素是某个父元素的子元素，选择子元素p，且子元素p必须是其父元素下的第二个p元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9d79f",
   "metadata": {},
   "source": [
    "#### ⑤属性查找"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6dabd7",
   "metadata": {},
   "source": [
    "查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一节点，所以中间不能加空格，否则会无法匹配到。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f69d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.select('a[class=\"software\"]'))\n",
    "print(soup.select('a[href=\"http://example.com/hadoop\"]'))\n",
    "print(soup.select('p a[href=\"http://example.com/hadoop\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以上的select方法返回的结果都是列表形式，可以以遍历的形式进行输出，然后用 get_text()方法来获取它的内容，实例如下：\n",
    "print(type(soup.select('title')))\n",
    "\n",
    "print(soup.select('title')[0].get_text())\n",
    "\n",
    "for title in soup.select('title'):\n",
    "      print(title.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e5f29",
   "metadata": {},
   "source": [
    "# 2.6 综合实例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469f20b",
   "metadata": {},
   "source": [
    "* 2.6.1 采集网页数据保存到文本文件\n",
    "* 2.6.2 采集网页数据保存到MySQL数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb0aa1",
   "metadata": {},
   "source": [
    "## 2.6.1 采集网页数据保存到文本文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb7ac2d",
   "metadata": {},
   "source": [
    "访问古诗文网站 https://so.gushiwen.org/mingju/， 会显示如图所示的页面，里面包含了很多名句，点击某一个名句（比如“山有木兮木有枝，心悦君兮君不知”），就会出现完整的古诗。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b8d59",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_4.png)\n",
    "\n",
    "\n",
    "![avatar](image/Chapter2_2_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744517a6",
   "metadata": {},
   "source": [
    "下面编写网络爬虫程序，爬取名句页面的内容，保存到一个文本文件中，然后，再爬取每个名句的完整古诗页面，把完整古诗保存到一个文本文件中。可以打开一个浏览器，访问要爬取的网页，然后在浏览器中查看网页源代码，找到诗句内容所在的位置，总结出它们共同的特征，就可以将它们全部提取出来了，具体实现代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a51868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_poem.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e378095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数1：请求网页\n",
    "def page_request(url, ua):\n",
    "    response = requests.get(url, headers=ua)\n",
    "    html = response.content.decode('utf-8')\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb3cfcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'}\n",
    "url = f\"https://so.gushiwen.cn/mingjus/default.aspx?page=1&tstr=&astr=&cstr=&xstr=\"\n",
    "html=page_request(url, ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06abc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数2：解析网页\n",
    "def page_parse(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    title = soup('title')\n",
    "    sentence = soup.select('div.left > div.sons > div.cont > a:nth-of-type(1)')\n",
    "    poet = soup.select('div.left > div.sons > div.cont > a:nth-of-type(2)')\n",
    "    sentence_list = []\n",
    "    href_list = []\n",
    "    for i in range(len(list(poet))):\n",
    "        temp = sentence[i].get_text() + \"---\" + poet[i].get_text()\n",
    "        sentence_list.append(temp)\n",
    "        href = sentence[i].get('href')\n",
    "        href_list.append(\"https://so.gushiwen.cn\" + href)\n",
    "    return [href_list, sentence_list]\n",
    "info_list = page_parse(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0038b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数3：写入文本文件\n",
    "def save_txt(info_list):\n",
    "    import json\n",
    "    with open(r'file/sentence.txt', 'a', encoding='utf-8') as txt_file:\n",
    "        for element in info_list[1]:\n",
    "            txt_file.write(json.dumps(element, ensure_ascii=False) + '\\n\\n')\n",
    "save_txt(info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86dc071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.子网页处理函数：进入并解析子网页/请求子网页\n",
    "def sub_page_request(info_list):\n",
    "    subpage_urls = info_list[0]\n",
    "    ua = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'}\n",
    "    sub_html = []\n",
    "    for url in subpage_urls:\n",
    "        html = page_request(url, ua)\n",
    "        sub_html.append(html)\n",
    "    return sub_html\n",
    "sub_html = sub_page_request(info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbae978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.子网页处理函数：解析子网页，爬取诗句内容\n",
    "def sub_page_parse(sub_html):\n",
    "    poem_list = []\n",
    "    for html in sub_html:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        poem = soup.select('div.left > div.sons > div.cont > div.contson')\n",
    "        # poem_text = poem.get_text()\n",
    "        # print(len(poem))\n",
    "        if (len(poem) > 0):\n",
    "            # print(poem[0].text)\n",
    "            poem_text = poem[0].text\n",
    "            poem_list.append(poem_text.strip())\n",
    "    return poem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.子网页处理函数：保存诗句到txt\n",
    "def sub_page_save(poem_list):\n",
    "    import json\n",
    "    with open(r'file/poems.txt', 'a', encoding='utf-8') as txt_file:\n",
    "        for element in poem_list:\n",
    "            txt_file.write(json.dumps(element, ensure_ascii=False) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407bd651",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************开始爬取古诗文网站******************\n",
      "开始解析第1页\n",
      "开始解析第2页\n",
      "开始解析第3页\n",
      "******************爬取完成*********************\n",
      "共爬取150个古诗词名句，保存在如下路径：file/sentence.txt\n",
      "共爬取150个古诗词，保存在如下路径：file/poem.txt\n"
     ]
    }
   ],
   "source": [
    "# parse_poem.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "# 函数1：请求网页\n",
    "def page_request(url, ua):\n",
    "    response = requests.get(url, headers=ua)\n",
    "    html = response.content.decode('utf-8')\n",
    "    return html\n",
    "\n",
    "\n",
    "# 函数2：解析网页\n",
    "def page_parse(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    title = soup('title')\n",
    "    sentence = soup.select('div.left > div.sons > div.cont > a:nth-of-type(1)')\n",
    "    poet = soup.select('div.left > div.sons > div.cont > a:nth-of-type(2)')\n",
    "    sentence_list = []\n",
    "    href_list = []\n",
    "    for i in range(len(list(poet))):\n",
    "        temp = sentence[i].get_text() + \"---\" + poet[i].get_text()\n",
    "        sentence_list.append(temp)\n",
    "        href = sentence[i].get('href')\n",
    "        href_list.append(\"https://so.gushiwen.cn\" + href)\n",
    "    return [href_list, sentence_list]\n",
    "\n",
    "\n",
    "# 函数3：写入文本文件\n",
    "def save_txt(info_list):\n",
    "    import json\n",
    "    with open(r'file/sentence.txt', 'a', encoding='utf-8') as txt_file:\n",
    "        for element in info_list[1]:\n",
    "            txt_file.write(json.dumps(element, ensure_ascii=False) + '\\n\\n')\n",
    "\n",
    "\n",
    "# 4.子网页处理函数：进入并解析子网页/请求子网页\n",
    "def sub_page_request(info_list):\n",
    "    subpage_urls = info_list[0]\n",
    "    ua = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'}\n",
    "    sub_html = []\n",
    "    for url in subpage_urls:\n",
    "        html = page_request(url, ua)\n",
    "        sub_html.append(html)\n",
    "    return sub_html\n",
    "\n",
    "\n",
    "# 5.子网页处理函数：解析子网页，爬取诗句内容\n",
    "def sub_page_parse(sub_html):\n",
    "    poem_list = []\n",
    "    for html in sub_html:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        poem = soup.select('div.left > div.sons > div.cont > div.contson')\n",
    "        # poem_text = poem.get_text()\n",
    "        # print(len(poem))\n",
    "        if (len(poem) > 0):\n",
    "            # print(poem[0].text)\n",
    "            poem_text = poem[0].text\n",
    "            poem_list.append(poem_text.strip())\n",
    "    return poem_list\n",
    "\n",
    "\n",
    "# 6.子网页处理函数：保存诗句到txt\n",
    "def sub_page_save(poem_list):\n",
    "    import json\n",
    "    with open(r'file/poems.txt', 'a', encoding='utf-8') as txt_file:\n",
    "        for element in poem_list:\n",
    "            txt_file.write(json.dumps(element, ensure_ascii=False) + '\\n\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"****************开始爬取古诗文网站******************\")\n",
    "    ua = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'}\n",
    "    for i in range(1, 4):\n",
    "        #         url = 'https://so.gushiwen.org/mingju/default.aspx?p=%d&c=&t='%(i)\n",
    "        url = f\"https://so.gushiwen.cn/mingjus/default.aspx?page={i}&tstr=&astr=&cstr=&xstr=\"\n",
    "        time.sleep(1)\n",
    "        html = page_request(url, ua)\n",
    "        info_list = page_parse(html)\n",
    "        save_txt(info_list)\n",
    "        # 处理子网页\n",
    "        print(\"开始解析第%d\" % (i) + \"页\")\n",
    "        # 开始解析名句子网页\n",
    "        sub_html = sub_page_request(info_list)\n",
    "        poem_list = sub_page_parse(sub_html)\n",
    "        sub_page_save(poem_list)\n",
    "\n",
    "    print(\"******************爬取完成*********************\")\n",
    "    print(\"共爬取%d\" % (\n",
    "                i * 50) + \"个古诗词名句，保存在如下路径：file/sentence.txt\")\n",
    "    print(\"共爬取%d\" % (i * 50) + \"个古诗词，保存在如下路径：file/poem.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11429142",
   "metadata": {},
   "source": [
    "## 2.6.2 采集网页数据保存到MySQL数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f5328",
   "metadata": {},
   "source": [
    "由于很多网站设计了反爬机制，会导致爬取网页失败，因此，这里直接采集一个本地网页文件web_demo.html，它记录了不同关键词的搜索次数排名，其内容如下：\n",
    "```\n",
    "<html>\n",
    "<head><title>搜索指数</title></head>\n",
    "<body>\n",
    "<table>\n",
    "<tr><td>排名</td><td>关键词</td><td>搜索指数</td></tr>\n",
    "<tr><td>1</td><td>大数据</td><td>187767</td></tr>\n",
    "<tr><td>2</td><td>云计算</td><td>178856</td></tr>\n",
    "<tr><td>3</td><td>物联网</td><td>122376</td></tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e34fb",
   "metadata": {},
   "source": [
    "参照Mysql安装手册，在Windows系统中启动MySQL服务进程，打开MySQL命令行客户端，执行如下SQL语句创建数据库和表：\n",
    "```\n",
    "mysql > CREATE DATABASE webdb;\n",
    "mysql > USE webdb;\n",
    "mysql> create table search_index(\n",
    "    -> id int,\n",
    "    -> keyword char(20),\n",
    "    -> number int);\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946ad44",
   "metadata": {},
   "source": [
    "编写网络爬虫程序，读取网页内容进行解析，并把解析后的数据保存到MySQL数据库中，具体代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_to_mysql.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 读取本地HTML文件\n",
    "def get_html():\n",
    "    path = 'file/web_demo.html'\n",
    "    htmlfile= open(path,'r')\n",
    "    html = htmlfile.read()\n",
    "    return html\n",
    "\n",
    "# 解析HTML文件\n",
    "def parse_html(html):\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    all_tr=soup.find_all('tr')[1:]\n",
    "    all_tr_list = []\n",
    "    info_list = []\n",
    "    for i in range(len(all_tr)):\n",
    "        all_tr_list.append(all_tr[i])\n",
    "    for element in all_tr_list:\n",
    "        all_td=element.find_all('td')\n",
    "        all_td_list = []\n",
    "        for j in range(len(all_td)):\n",
    "            all_td_list.append(all_td[j].string)\n",
    "        info_list.append(all_td_list)\n",
    "    return info_list\n",
    "\n",
    "# 保存数据库\n",
    "def save_mysql(info_list):\n",
    "    import pymysql.cursors\n",
    "    # 连接数据库\n",
    "    connect = pymysql.Connect(\n",
    "        host='localhost',\n",
    "        port=3306,\n",
    "        user='root',  # 数据库用户名\n",
    "        passwd='123456',  # 密码\n",
    "        db='webdb',\n",
    "        charset='utf8'\n",
    "    )\n",
    "\n",
    "    # 获取游标\n",
    "    cursor = connect.cursor()\n",
    "\n",
    "    # 插入数据\n",
    "    for item in info_list:\n",
    "        id = int(item[0])\n",
    "        keyword = item[1]\n",
    "        number = int(item[2])\n",
    "        sql = \"INSERT INTO search_index(id,keyword,number) VALUES ('%d', '%s', %d)\"\n",
    "        data = (id,keyword,number)\n",
    "        cursor.execute(sql % data)\n",
    "        connect.commit()\n",
    "    print('成功插入数据')\n",
    "\n",
    "    # 关闭数据库连接\n",
    "    connect.close()\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    html = get_html()\n",
    "    info_list = parse_html(html)\n",
    "    save_mysql(info_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a160578",
   "metadata": {},
   "source": [
    "执行代码文件，然后到MySQL命令行客户端执行如下SQL语句查看数据：  \n",
    "`mysql> select * from search_index;`  \n",
    "可以看到，有3条数据被成功插入了数据库中。  \n",
    "![avatar](image/Chapter2_2_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86939be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
